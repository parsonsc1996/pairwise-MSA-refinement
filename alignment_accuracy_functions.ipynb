{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairwise-bootstrapping?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import ete3\n",
    "\n",
    "import os \n",
    "\n",
    "alphabet = {\"-\":0, \"A\": 1, \"R\": 2, \"N\":3, \"D\":4, \"C\":5, \"Q\":6, \"E\":7, \"G\":8,\n",
    "            \"H\":9, \"I\":10, \"L\":11, \"K\":12, \"M\":13, \"F\":14, \"P\":15, \n",
    "            \"S\":16, \"T\":17, \"W\":18, \"Y\":19, \"V\":20, \"X\":21}\n",
    "\n",
    "import math\n",
    "INF = math.inf\n",
    "\n",
    "import random\n",
    "\n",
    "from Bio import Align\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_seq_dict(fasta_path):\n",
    "    seqDict = {}\n",
    "    with open(fasta_path) as f:\n",
    "        cur = \"\"\n",
    "        for line in f:\n",
    "            if line[0] == \">\":\n",
    "                cur = line[1:].strip()\n",
    "                seqDict[cur] = \"\"\n",
    "            else:\n",
    "                seqDict[cur] += line.strip().replace(\"X\",\"-\")\n",
    "    return seqDict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_dict_to_matrix(seqDict):\n",
    "    num_seqs = len(seqDict)\n",
    "    alignment_len = len(list(seqDict.values())[0])\n",
    "    labels = np.empty((num_seqs, 1), dtype=object)\n",
    "    sequence_matrix = np.empty((num_seqs, alignment_len), dtype=object)\n",
    "    i = 0\n",
    "    for label in seqDict:\n",
    "        labels[i, 0] = label\n",
    "        seq = seqDict[label]\n",
    "        sequence_matrix[i] = np.array([x.strip() for x in seq])\n",
    "        i += 1\n",
    "    return sequence_matrix, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeShell(name, commands, nodes='1', cores='1', mem='10', days='7', out=None):\n",
    "    if out == None:\n",
    "        out = name\n",
    "    file = open(name + '.sh', 'w')\n",
    "    file.write( \"#!/bin/bash\\n\\n\"\n",
    "                \"#SBATCH -p sched_mit_g4nier\\n\"                                                                          \n",
    "                \"#SBATCH -t \" + days + \"-00:00:00\\n\"\n",
    "                \"#SBATCH -N \" + nodes + \"\\n\"\n",
    "                \"#SBATCH -n \" + cores + \"\\n\"\n",
    "                \"#SBATCH --mem=\" + mem + \"G\\n\"\n",
    "                \"#SBATCH -J \" + name + \"\\n\"\n",
    "                \"#SBATCH -o \" + out + \".out\\n\" +\n",
    "                '\\n'.join(commands))\n",
    "    file.close()\n",
    "    return name + '.sh'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(alignmentpath, name):\n",
    "    commands = [\"module add engaging/iqtree/1.6.3\",\n",
    "                \"iqtree -s \" + alignmentpath + \n",
    "                \" -nt 5 -bb 1000 -alrt 1000 -m MFP -mset WAG,LG,JTT -msub nuclear\"]\n",
    "    iqtreeSh = makeShell(name, commands, mem = \"10\", cores=\"5\")\n",
    "    subprocess.run([\"sbatch\", iqtreeSh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supports(tree_path, metric=None):\n",
    "    t = ete3.Tree(tree_path, format=1)\n",
    "    tot = []\n",
    "    for node in t.traverse():\n",
    "        if node.is_leaf() or node.name==\"\":\n",
    "            continue\n",
    "        #print(node.name)\n",
    "        alrt, bb = node.name.split(\"/\")\n",
    "        if metric==\"alrt\":\n",
    "            tot.append(float(alrt))\n",
    "        elif metric==\"bb\":\n",
    "            tot.append(float(bb))\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique(items):\n",
    "    unique_items = []\n",
    "    for item in items:\n",
    "        if item not in unique_items:\n",
    "            unique_items.append(item)\n",
    "    return unique_items\n",
    "\n",
    "def vector_angle(v1, v2):\n",
    "    v1_norm, v2_norm = v1/np.linalg.norm(v1), v2/np.linalg.norm(v2)\n",
    "    return 180 * np.arccos(np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)) / np.pi\n",
    "\n",
    "def cos_vector_angle(v1, v2):\n",
    "    v1_norm, v2_norm = v1/np.linalg.norm(v1), v2/np.linalg.norm(v2)\n",
    "    return np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n",
    "\n",
    "def cos_vector_angle_exclude(v1, v2, exclude=0):\n",
    "    #print(v1.shape, v2.shape)\n",
    "    if v1[exclude] == 1 or v2[exclude] == 1:\n",
    "        return cos_vector_angle(v1, v2)\n",
    "    new_v1, new_v2 = np.concatenate((v1[:exclude],v1[exclude+1:])), np.concatenate((v2[:exclude],v2[exclude+1:]))\n",
    "    return cos_vector_angle(new_v1, new_v2)\n",
    "\n",
    "def cos_vector_angle_dilute(v1, v2, dilute=0):\n",
    "    #print(v1.shape, v2.shape)\n",
    "    if v1[dilute] == 1 or v2[dilute] == 1:\n",
    "        return cos_vector_angle(v1, v2)\n",
    "    v1_gaps, v2_gaps = v1[dilute], v2[dilute]\n",
    "    redistribute_v1, redistribute_v2 = v1.copy(), v2.copy()\n",
    "    redistribute_v1 = redistribute_v1 * (((1-v1_gaps)+(v1_gaps/2))/(1-v1_gaps))\n",
    "    redistribute_v2 = redistribute_v2 * (((1-v2_gaps)+(v2_gaps/2))/(1-v2_gaps))\n",
    "    redistribute_v1[dilute] = v1_gaps/2\n",
    "    redistribute_v2[dilute] = v2_gaps/2\n",
    "    return cos_vector_angle(redistribute_v1, redistribute_v2)\n",
    "\n",
    "def id_poor_sites(alignmentFrequencyMatrix, sequenceFrequencyMatrix, gapDistribution, \n",
    "                  metric=cos_vector_angle, threshold=0.2):\n",
    "    scores = get_site_fit_metrics(alignmentFrequencyMatrix, sequenceFrequencyMatrix, gapDistribution, metric=metric)\n",
    "    #print(scores, len(scores))\n",
    "    return np.argwhere(scores<threshold)[:,0]\n",
    "\n",
    "def get_site_fit_metrics(alignmentFrequencyMatrix, sequenceFrequencyMatrix, gapDistribution, metric=vector_angle):\n",
    "    alphabet_size, alignment_len = alignmentFrequencyMatrix.shape\n",
    "    alphabet_size, sequence_len = sequenceFrequencyMatrix.shape\n",
    "    site = 0\n",
    "    scores = []\n",
    "    for i in range(alignment_len):\n",
    "        if gapDistribution[i] == \"-\":\n",
    "            scores.append(INF)\n",
    "        #print(gapDistribution[i])\n",
    "        else:\n",
    "            scores.append(metric(sequenceFrequencyMatrix[:, site], alignmentFrequencyMatrix[:, i]))\n",
    "            site += 1\n",
    "    #print([x for x in scores if x != INF])\n",
    "    return np.array(scores)\n",
    "\n",
    "def get_aa_frequencies_by_site(alignmentMatrix, alphabet=alphabet):\n",
    "    num_seqs, alignment_len = alignmentMatrix.shape\n",
    "    alphabet_size = len(alphabet)\n",
    "    frequencyMatrix = np.zeros((alphabet_size, alignment_len))\n",
    "    for i in range(alignment_len):\n",
    "        for j in range(num_seqs):\n",
    "            frequencyMatrix[alphabet[alignmentMatrix[j,i]], i] += 1\n",
    "    return frequencyMatrix / num_seqs\n",
    "\n",
    "def modifyAlignmentBasedOnPairwiseFrequencies(alignmentMatrix, pairwiseFrequencies, \n",
    "                                              delete_bad=False, metric=cos_vector_angle, threshold=0.2,\n",
    "                                              max_deletions_per_col=1, max_deletions_per_seq=1, \n",
    "                                              max_gaps_per_col=1):\n",
    "    alignmentFrequencyMatrix = get_aa_frequencies_by_site(alignmentMatrix)\n",
    "    updated_alignmentMatrix = np.empty(alignmentMatrix.shape, dtype=object)\n",
    "    for i in range(len(pairwiseFrequencies)):\n",
    "        sequenceFrequencyMatrix = pairwiseFrequencies[i].T\n",
    "        sequence = alignmentMatrix[i,:]\n",
    "        #print(alignmentFrequencyMatrix.shape, sequence.shape, sequenceFrequencyMatrix.shape, len(sequence[sequence!=\"-\"]))\n",
    "        poor_sites = id_poor_sites(alignmentFrequencyMatrix, sequenceFrequencyMatrix, \n",
    "                                   sequence, metric=metric, threshold=threshold)\n",
    "        new_sequence = sequence.copy()\n",
    "        for site in poor_sites:\n",
    "            if delete_bad:\n",
    "                new_sequence[site] = \"X\"\n",
    "            else:\n",
    "                #print(new_sequence[site])\n",
    "                new_sequence[site] = new_sequence[site].lower()\n",
    "        #print(new_sequence.tolist())\n",
    "        updated_alignmentMatrix[i,:] = new_sequence\n",
    "    return updated_alignmentMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignment_score_matrix(seq, align_freq, alphabet=alphabet, \n",
    "                                    substitution_matrix=None, exclude=[0]):\n",
    "    if substitution_matrix == None:\n",
    "        substitution_matrix = Align.substitution_matrices.load(\"BLOSUM62\")\n",
    "    seq_len = len(seq)\n",
    "    align_len = align_freq.shape[1]\n",
    "    score_matrix = np.zeros((seq_len, align_len))\n",
    "    inv_alphabet = {v: k for k, v in alphabet.items()}\n",
    "    for s in range(seq_len):\n",
    "        site_in_seq = alphabet[seq[s]]\n",
    "        partner_score_dict = substitution_matrix[site_in_seq]\n",
    "        if (site_in_seq in exclude):\n",
    "            score_matrix[s,a] = np.min(substitution_matrix)\n",
    "            continue\n",
    "        site_in_seq = inv_alphabet[site_in_seq]\n",
    "        #print(site_in_seq)\n",
    "        for a in range(align_len):\n",
    "            if (s > a) or ((align_len - a) < (seq_len - s)):\n",
    "                score_matrix[s,a] = np.min(substitution_matrix)\n",
    "                continue\n",
    "\n",
    "            site_freqs_in_align = [align_freq[x, a] for x in range(align_freq.shape[0]) if x not in exclude]\n",
    "            partner_scores = [partner_score_dict[x] for x in range(align_freq.shape[0]) if x not in exclude]\n",
    "            #print(site_in_align, site_in_seq, a,  s)\n",
    "            site_freqs_in_align = site_freqs_in_align / np.sum(site_freqs_in_align)\n",
    "            #print(np.sum(site_freqs_in_align))\n",
    "            #print(partner_scores, site_freqs_in_align, np.sum([partner_scores[x] * site_freqs_in_align[x] for x in range(len(partner_scores))]) + 5)\n",
    "            score_matrix[s,a] = (np.sum([partner_scores[x] * site_freqs_in_align[x] for x in range(len(partner_scores))]) - np.min(substitution_matrix))\n",
    "        site_max = np.max(score_matrix[s,a])\n",
    "        if site_max != site_max:\n",
    "            score_matrix[s,a] = score_matrix[s,a] / site_max\n",
    "        #print(score_matrix[s,:])\n",
    "        #score_matrix[s,:] = (score_matrix[s,:] - np.min(score_matrix)) / (np.max(partner_score_dict) - np.min(partner_score_dict))\n",
    "        #score_matrix[s,:] = (score_matrix[s,:] - np.min(score_matrix)) / (np.max(partner_score_dict) - np.min(partner_score_dict))\n",
    "        #print(score_matrix[s,:])\n",
    "    #print(np.max(score_matrix), np.min(score_matrix))\n",
    "    print(score_matrix.shape)\n",
    "    score_matrix = (score_matrix - np.min(score_matrix[score_matrix!=np.min(substitution_matrix)])) / (np.max(score_matrix) - np.min(score_matrix[score_matrix!=np.min(substitution_matrix)]))\n",
    "    print(score_matrix.shape)\n",
    "    score_matrix[score_matrix==np.min(score_matrix)] = 0\n",
    "    print(score_matrix.shape)\n",
    "    return score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_dictionary(matrix, labels):\n",
    "    num_seqs, num_sites = matrix.shape\n",
    "    dictionary = {}\n",
    "    for i in range(num_seqs):\n",
    "        dictionary[labels[i][0]] = \"\".join(matrix[i,:])\n",
    "    return dictionary\n",
    "\n",
    "def get_match_matrix(frequencyMatrix, alignmentFrequencies, metric=cos_vector_angle):\n",
    "    alphabet_size, align_length = alignmentFrequencies.shape\n",
    "    seq_length, alphabet_size = frequencyMatrix.shape\n",
    "    match_matrix = np.zeros((seq_length, align_length))\n",
    "    #print(match_matrix.shape)\n",
    "    for s in range(seq_length):\n",
    "        for a in range(align_length):\n",
    "            if s < a and (align_length - a) > (seq_length - s):\n",
    "                score = metric(frequencyMatrix[s,:], alignmentFrequencies[:, a])\n",
    "                match_matrix[s, a] = score\n",
    "    return match_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pathfind(match_matrix, true_random=False, fixed_sites=None):\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    cur = np.random.choice(list(range(align_length - seq_length)), \n",
    "                           p=match_matrix[0,:align_length-seq_length]/np.sum(match_matrix[0,:align_length-seq_length]))\n",
    "    scores = []\n",
    "    inds = []\n",
    "    #print(fixed_sites, check_monotonic(fixed_sites))\n",
    "    if true_random:\n",
    "        sites = np.random.choice(list(range(align_length)), size=seq_length, replace=False)\n",
    "        sites = [x for x in np.sort(sites)]\n",
    "        scores = [match_matrix[s, sites[s]] for s in range(seq_length)]\n",
    "        return scores, np.sum(scores), sites\n",
    "    \n",
    "    if fixed_sites != None:\n",
    "        sites = []\n",
    "        last_s, last_a = -1, -1\n",
    "        for site in fixed_sites:\n",
    "            s, a = site\n",
    "            if last_a+1 != a:\n",
    "                region_sites = np.random.choice(list(range(last_a+1, a)), replace=False, size=s-last_s-1)#, p=site_strengths)\n",
    "                if s-last_s-1 == 1:\n",
    "                    region_sites = [int(region_sites)]\n",
    "            else:\n",
    "                region_sites = []\n",
    "            region_sites = list(np.sort(region_sites)) + [a]\n",
    "            last_s, last_a = s, a \n",
    "            sites += list(region_sites)\n",
    "        if last_a != align_length-1:\n",
    "            region_sites = [x for x in np.random.choice(list(range(last_a+1, align_length)), replace=False, size=seq_length-last_s-1)]\n",
    "            region_sites = np.sort(region_sites)\n",
    "            sites += list(region_sites)\n",
    "        scores = [match_matrix[s, sites[s]] for s in range(seq_length)]\n",
    "        #print([(x[0], x[1], sites[x[0]]) for x in fixed_sites], len(sites), seq_length)\n",
    "        return scores, np.sum(scores), sites\n",
    "\n",
    "    for s in range(seq_length):\n",
    "        good_indices = np.argwhere([match_matrix[s,:]>np.random.random() * 0.5])[:,1]\n",
    "        good_indices = good_indices[good_indices>s]\n",
    "        #adjacent = match_matrix[s,cur+1]\n",
    "        best = (None, 0)\n",
    "        root = 0.8 * ((np.random.random()) ** 0.5)\n",
    "        candidates = good_indices[good_indices>cur]\n",
    "        for a in range(min([int(np.random.randint(0, 25)**root), len(candidates)])):\n",
    "            next_ind = good_indices[good_indices>cur][a]\n",
    "            #print(min((next_ind - cur), 10))\n",
    "            upgrade = match_matrix[s,next_ind] * (root ** min((next_ind - cur - 1), 1))\n",
    "            if upgrade > best[1]:\n",
    "                best = (next_ind, upgrade)\n",
    "        if best[0] == None:\n",
    "            cur = cur + 1\n",
    "        elif best[0] < np.random.random() * 0.5:\n",
    "            cur = good_indices[good_indices>cur][0]\n",
    "        else:\n",
    "            cur = best[0]\n",
    "        scores.append(match_matrix[s,cur])\n",
    "        inds.append(cur)\n",
    "    return scores, np.sum(scores), inds\n",
    "        \n",
    "#No idea if this works. All my datasets are way too big for this to finish running anyway\n",
    "def recursive_pathfind(match_matrix):\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    if seq_length == 1 and align_length == 1:\n",
    "        return match_matrix[0][0]\n",
    "    elif seq_length == 0 and align_length == 0:\n",
    "        return 0\n",
    "    good_candidates = np.argwhere([match_matrix>0.8])\n",
    "    #print(good_candidates[np.random.randint(len(good_candidates))])\n",
    "    _, s, a = good_candidates[np.random.randint(len(good_candidates))]\n",
    "    if s == seq_length - 1 or a == align_length - 1:\n",
    "        return recursive_pathfind(match_matrix[:s,:a]) + match_matrix[s,a]\n",
    "    elif s == 0 or a == 0:\n",
    "        return match_matrix[s,a] + recursive_pathfind(match_matrix[s+1:,a+1:])\n",
    "    else:\n",
    "        print(s, a, match_matrix.shape)\n",
    "        print(match_matrix[s,a])\n",
    "        return recursive_pathfind(match_matrix[:s,:a]) + match_matrix[s,a] + recursive_pathfind(match_matrix[s+1:,a+1:])\n",
    "\n",
    "    \n",
    "def depth_first_pathfinding(match_matrix, best_score=-INF):\n",
    "    stack = [[]]\n",
    "    best_final = [best_score, []]\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    while stack:\n",
    "        state = stack.pop()\n",
    "        if len(state) < align_length:\n",
    "            remainder_alignment = align_length - len(state)\n",
    "            remainder_sequence = seq_length - sum(state)\n",
    "            print(len(stack), len(state), best_final[0])\n",
    "            clear_output(wait=True)\n",
    "            #print(evaluate_score(match_matrix, state) + remainder)\n",
    "            if evaluate_score(match_matrix, state) + remainder_sequence > best_final[0]:\n",
    "                if np.random.random() < 0.5:\n",
    "                    if remainder_alignment > remainder_sequence:\n",
    "                        stack.append(state.copy() + [0])\n",
    "                    if remainder_sequence > 0:\n",
    "                        stack.append(state.copy() + [1])\n",
    "                else:\n",
    "                    if remainder_sequence > 0:\n",
    "                        stack.append(state.copy() + [1])\n",
    "                    if remainder_alignment > remainder_sequence:\n",
    "                        stack.append(state.copy() + [0])\n",
    "\n",
    "        else:\n",
    "            score = evaluate_score(match_matrix, state)\n",
    "            if score > best_final[0]:\n",
    "                best_final = [score, state]\n",
    "    return best_final\n",
    "            \n",
    "def evaluate_score(match_matrix, place_skip_record):\n",
    "    s = 0\n",
    "    score = 0\n",
    "    for a in range(len(place_skip_record)):\n",
    "        place_skip = place_skip_record[a]\n",
    "        if place_skip == 1:\n",
    "            score += match_matrix[s, a]\n",
    "            s += 1\n",
    "    return score\n",
    "\n",
    "def amalgamate(paths, scores, consensus=0.8):\n",
    "    score_matrix = np.vstack(scores)\n",
    "    path_matrix = np.vstack(paths)\n",
    "    best = [max([sum(x) for x in scores]), max([x for x in range(len(scores))], key=lambda x: sum(scores[x]))]\n",
    "    #print(best, paths[best[1]])\n",
    "\n",
    "def perturb(match_matrix, paths, scores, mu, fixed_sites=[]):\n",
    "    paths, scores = paths.copy(), scores.copy()\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    fixed_sites = {s:a for s,a in fixed_sites}\n",
    "    for p in range(len(paths)):\n",
    "        path = paths[p]\n",
    "        score = scores[p]\n",
    "        scrambled = list(range(len(path)))\n",
    "        np.random.shuffle(scrambled)\n",
    "        for s in scrambled:\n",
    "            new_match = path[s]\n",
    "            if s == 0:\n",
    "                if path[1] == 1:\n",
    "                    continue\n",
    "                if np.random.random() < mu:\n",
    "                    new_match = np.random.randint(0, path[1])\n",
    "                    #new_match = np.random.choice(range(0, path[1]), 1, match_matrix[s][0, path[1]])\n",
    "            elif s == len(path) - 1:\n",
    "                if path[-2] == align_length - 2:\n",
    "                    continue\n",
    "                if np.random.random() < mu:\n",
    "                    new_match = np.random.randint(path[-2] + 1, align_length)\n",
    "            else:\n",
    "                try:\n",
    "                    if np.random.random() < mu:\n",
    "                        new_match = np.random.randint(path[s-1] + 1, path[s+1])\n",
    "                except:\n",
    "                    a=1\n",
    "                    #print(s, new_match, path, match_matrix.shape)\n",
    "            try:\n",
    "                if match_matrix[s, new_match] > match_matrix[s, path[s]]:# or np.random.random() > 0.99:\n",
    "                    if s not in fixed_sites:\n",
    "                        path[s] = new_match\n",
    "                        score[s] = match_matrix[s, path[s]]\n",
    "            except:\n",
    "                a=1\n",
    "                #print(s, new_match, path, match_matrix.shape)\n",
    "        paths[p] = path\n",
    "        scores[p] = score\n",
    "    return paths, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# starts is a collection of the best starts at particular indices of the sequence\n",
    "# ends is the set of best matches to the corresponding start in starts\n",
    "def get_best_start_ends(population, match_matrix, prev_dicts=None):\n",
    "    population=population.copy()\n",
    "    match_matrix=match_matrix.copy()\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    if prev_dicts == None or prev_dicts[0] == None or prev_dicts[1] == None:\n",
    "        starts={x:[] for x in range(seq_length+1)}\n",
    "        ends={x:[] for x in range(seq_length+1)}\n",
    "    else:\n",
    "        starts = prev_dicts[0]\n",
    "        ends = prev_dicts[1]\n",
    "    pop_size = len(population)\n",
    "    for s in range(seq_length+1):\n",
    "        #print(s)\n",
    "        #clear_output(wait=True)\n",
    "        starts_s = []\n",
    "        ends_s = []\n",
    "        for i in range(pop_size):\n",
    "            start_scores = [match_matrix[j,population[i][:s][j]] for j in range(s)]\n",
    "            end_scores = [match_matrix[s+j,population[i][s:][j]] for j in range(seq_length-s)]\n",
    "            start = population[i][:s]\n",
    "            end = population[i][s:]\n",
    "            #if (start, start_scores) not in starts[s] + starts_s:\n",
    "            starts_s.append((start, start_scores))\n",
    "            #if (end, end_scores) not in ends[s] + ends_s:\n",
    "            ends_s.append((end, end_scores))\n",
    "        if starts[s] == None or ends[s] == None:\n",
    "            starts[s], ends[s] = [], []\n",
    "        starts[s] += starts_s\n",
    "        ends[s] += ends_s\n",
    "        starts[s] = sorted(starts[s], key=lambda x: np.sum(x[1]))\n",
    "        ends[s] = sorted(ends[s], key=lambda x: np.sum(x[1]))\n",
    "        starts[s].reverse()\n",
    "        ends[s].reverse()\n",
    "        \n",
    "        starts[s] = make_unique(starts[s])\n",
    "        ends[s] = make_unique(ends[s])\n",
    "        ends_s = []\n",
    "        for start in starts[s]:\n",
    "            if (len(start[0]) == 0) or (len(start[0]) == seq_length):\n",
    "                compatible = ends[s]\n",
    "            else:  \n",
    "                #print([end[0][0] for end in ends[s]])\n",
    "                #print(start)\n",
    "                compatible = [end for end in ends[s] if end[0][0] > start[0][-1]]\n",
    "                #print([(start[0][-1], end[0][0]) for end in ends[s] if end[0][0] > start[0][-1]])\n",
    "            #print(len(compatible))\n",
    "            if len(compatible) > 0:\n",
    "                ends_s.append(compatible[0])\n",
    "        ends[s] = ends_s\n",
    "        #print(ends[s])\n",
    "        starts[s] = starts[s][:min(len(starts[s]), pop_size)]\n",
    "        ends[s] = ends[s][:min(len(ends[s]), pop_size)]\n",
    "    return starts, ends\n",
    "\n",
    "def generate_new_pop_from_starts_ends(starts, ends, pop_size):\n",
    "    new_pop = []\n",
    "    starts = starts.copy()\n",
    "    ends = ends.copy()\n",
    "    seq_len = len(starts)\n",
    "    #random_inds = [np.random.randint(1, seq_len - 1) for x in range(pop_size // 2)]\n",
    "    #random_guys = [(starts[random_inds[x]][0][0] + ends[random_inds[x]][0][0], \n",
    "    #                starts[x][0][1] + ends[x][0][1]) for x in range(pop_size // 2)]\n",
    "    #print(ends)\n",
    "    #print(starts)\n",
    "    sorted_by_score = [(starts[x][0][0] + ends[x][0][0], \n",
    "                        starts[x][0][1] + ends[x][0][1]) for x in range(1, seq_len - 1)]\n",
    "    sorted_by_score = sorted(sorted_by_score, key=lambda x: np.sum(x[1]))\n",
    "    sorted_by_score.reverse()\n",
    "    sorted_by_score = make_unique(sorted_by_score)\n",
    "    #print([x[1] for x in sorted_by_score])\n",
    "    #combined = sorted_by_score[:pop_size - (pop_size // 2)] + random_guys\n",
    "    combined = sorted_by_score[:min(pop_size, len(sorted_by_score))]\n",
    "    combined = [[sum(scores), scores, path] for path, scores in combined]\n",
    "    return combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diverse_initial_population(heatmap, pop_size=50, update_freq=100, max_rounds=1000, verbose=False, fixed_sites=None):\n",
    "    best_scores_per_site = [[0] * heatmap.shape[0]]\n",
    "    starts, ends = None, None\n",
    "    prev = 0\n",
    "    mean = 0\n",
    "    best = 0\n",
    "    min_score = 0\n",
    "    good = []\n",
    "    all_seen = []\n",
    "    fixed_sites = fixed_sites.copy()\n",
    "    site_ranges = []\n",
    "    for s in range(heatmap.shape[0]):\n",
    "        if s in [x[0] for x in fixed_sites]:\n",
    "            limit = {x[0]:x[1] for x in fixed_sites}[s]\n",
    "            site_ranges.append((limit, limit))\n",
    "        else:\n",
    "            upper_limits = [x for x in fixed_sites if x[0] > s] + [[heatmap.shape[0], heatmap.shape[1]]]\n",
    "            lower_limits = [x for x in fixed_sites if x[0] < s] + [[0, 0]]\n",
    "            site_ranges.append((max([x[1] for x in lower_limits]), min([x[1] for x in upper_limits])))\n",
    "    #print(site_ranges)\n",
    "    best_a_by_s = []\n",
    "    for s in range(heatmap.shape[0]):\n",
    "        candidates = heatmap[s, site_ranges[s][0]:site_ranges[s][1]]\n",
    "        if candidates != []:\n",
    "            best_a_by_s.append(int(np.argmax(candidates)) + site_ranges[s][0])\n",
    "        else:\n",
    "            best_a_by_s.append(site_ranges[s][0])\n",
    "    best_a_by_s = [(x, best_a_by_s[x]) for x in range(heatmap.shape[0])]\n",
    "    #print(best_a_by_s)\n",
    "    #print(len(fixed_sites))\n",
    "    for i in range(max_rounds + 1):\n",
    "        #print(best_a_by_s, fixed_sites)\n",
    "        random_fixed = get_mutually_compatible(heatmap, best_a_by_s, fixed_sites)\n",
    "        #print(random_fixed)#, best_a_by_s)\n",
    "        np.random.shuffle(random_fixed)\n",
    "        #print(random_fixed)\n",
    "        #random_fixed = random_fixed[:np.random.randint(len(random_fixed)-1)]\n",
    "        fixed_round = make_unique(sorted(fixed_sites + random_fixed, key=lambda x:x[0]))\n",
    "        #print(len(random_fixed), len(fixed_sites))\n",
    "        #print(check_monotonic(fixed_round))\n",
    "        scores, score, inds = random_pathfind(heatmap, true_random=False, fixed_sites=fixed_round)\n",
    "        #score = recursive_pathfind(heatmap)\n",
    "        #score, path = depth_first_pathfinding(heatmap, best_score=np.sum([heatmap[s, seq_to_align[s]] for s in range(seq_length)]))\n",
    "        #print(i)\n",
    "        if (i % update_freq == 0) or (i == 1):\n",
    "            if i != 0:\n",
    "                mean = round(np.mean([x for x,_,_ in good]), 2)\n",
    "                #paths = [g[2] for g in good]\n",
    "                #print(paths)\n",
    "                #if starts == None:\n",
    "                #    starts, ends = get_best_start_ends(all_seen, heatmap, prev_dicts=None)\n",
    "                #    all_seen = []\n",
    "                #else:\n",
    "                #    starts, ends = get_best_start_ends(all_seen, heatmap, prev_dicts=(starts, ends))\n",
    "                #    all_seen = []\n",
    "                if mean != np.nan and ((mean - prev) / prev) < (0.001) and i != update_freq:\n",
    "                    prev = mean\n",
    "                    print(\"Converged on round\", i, \"\\nBest:\", round(best, 2))\n",
    "                    starts, ends = get_best_start_ends(all_seen, heatmap, prev_dicts=(starts, ends))\n",
    "                    while len(good) > pop_size:\n",
    "                        min_ind = min([x for x in range(len(good))], key=lambda x: sum(good[x][1]))\n",
    "                        good.pop(min_ind)\n",
    "\n",
    "                    return good, starts, ends\n",
    "                else:\n",
    "                    best = round(max([x for x,_,_ in good]), 2)\n",
    "                    if verbose:\n",
    "                        clear_output(wait=True)\n",
    "                        #print(\"Score to beat:\", alignment_score, \"/\", seq_length, \n",
    "                        #      \"... Done!\" if best > alignment_score else \"...\")\n",
    "                        print(\"Not converged after\", i, \"rounds\\nBest:\", round(best, 2),\n",
    "                              \"/\", heatmap.shape[0], \"Improvement:\", \n",
    "                              round(100 * (mean - prev) / mean, 2), \"%\", \"Mean: \", mean)\n",
    "                        print(len(all_seen), \"paths found\")\n",
    "                    prev = mean\n",
    "\n",
    "\n",
    "        if score:\n",
    "            for i in range(heatmap.shape[0]):\n",
    "                best_scores_per_site.append([max(scores[x], best_scores_per_site[-1][x]) for x in range(len(scores))])\n",
    "            if inds in all_seen or score < best * 0.95:\n",
    "                continue\n",
    "            else:\n",
    "                all_seen.append(inds)\n",
    "            if score > best:\n",
    "                #if verbose and best < alignment_score and score > alignment_score:\n",
    "                #    print(\"Alignment beaten!\")\n",
    "                best = score\n",
    "                #print(round(best,2), len(good), round(np.mean([x for x,_,_ in good]), 2))\n",
    "                if len(good) > 0:\n",
    "                    good.pop(min_ind)\n",
    "                good.append([score, scores, inds])\n",
    "                min_score = min([x for x,_,_ in good])\n",
    "                min_ind = min([x for x in range(len(good))], key=lambda x: sum(good[x][1]))\n",
    "            elif len(good) < pop_size:\n",
    "                good.append([score, scores, inds])\n",
    "                min_score = min([x for x,_,_ in good])\n",
    "                min_ind = min([x for x in range(len(good))], key=lambda x: sum(good[x][1]))\n",
    "            elif score > min_score:\n",
    "                #print(score)\n",
    "                good.pop(min_ind)\n",
    "                good.append([score, scores, inds])\n",
    "                min_score = min([x for x,_,_ in good])\n",
    "                min_ind = min([x for x in range(len(good))], key=lambda x: sum(good[x][1]))\n",
    "    good = sorted(good, key=lambda x:x[0])\n",
    "    good.reverse()\n",
    "    good = good[:min(pop_size,len(good))]\n",
    "    #while len(good) > pop_size:\n",
    "    #    min_ind = min([x for x in range(len(good))], key=lambda x: sum(good[x][1]))\n",
    "    #    good.pop(min_ind)\n",
    "\n",
    "    starts, ends = get_best_start_ends(all_seen, heatmap, prev_dicts=None)\n",
    "    return good, starts, ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_refine(match_matrix, population, max_generations=200, verbose=False, fixed_sites=None):\n",
    "    if verbose:\n",
    "        print(\"Refining...\")\n",
    "    paths = [x[2].copy() for x in population]\n",
    "    scores = [x[1].copy() for x in population]\n",
    "    upgraded = []\n",
    "    prev_j = 0\n",
    "    starting = max([sum(x) for x in scores])\n",
    "    for i in range(max_generations):\n",
    "        p = [x.copy() for x in paths]\n",
    "        s = [x.copy() for x in scores]\n",
    "        num_rounds = np.random.randint(prev_j * 2 + 3)\n",
    "        if num_rounds > 100:\n",
    "            num_rounds = 3\n",
    "        for j in range(num_rounds):\n",
    "            if fixed_sites == None:\n",
    "                p, s = perturb(match_matrix, p, s, 0.9)\n",
    "            else:\n",
    "                p, s = perturb(match_matrix, p, s, 0.9, fixed_sites=fixed_sites)\n",
    "            round_best = max([sum(x) for x in s])\n",
    "            round_best_ind = max([x for x in range(len(s))], key=lambda x: sum(s[x]))\n",
    "            round_best_value = max([sum(s[x]) - sum(scores[x]) for x in range(len(s))])\n",
    "            round_best_value_ind = max([x for x in range(len(s))], key=lambda x: sum(s[x]) - sum(scores[x]))\n",
    "            #round_worst = min([sum(x) for x in s])\n",
    "            round_worst_ind = min([x for x in range(len(s))], key=lambda x: sum(s[x]))\n",
    "            if round_best>sum(scores[round_best_ind]):\n",
    "                pre = sum(scores[round_best_ind])\n",
    "                paths[round_worst_ind] = paths[round_best_ind].copy()\n",
    "                scores[round_worst_ind] = scores[round_best_ind].copy()\n",
    "                paths[round_best_ind] = p[round_best_ind].copy()\n",
    "                scores[round_best_ind] = s[round_best_ind].copy()\n",
    "                clear_output(wait=True)\n",
    "                upgraded.append(round_best_ind)\n",
    "                if verbose:\n",
    "                    print(\"generation:\", i, \"round:\", j, \"of\", num_rounds, \n",
    "                          round(len(set(upgraded)) / len(paths)*100,2), \"% upgraded\")\n",
    "                    print(round(pre, 2), \"->\", round(round_best, 2), \"best:\",\n",
    "                          round(np.max([sum(x) for x in scores]), 2), \"mean:\",\n",
    "                          round(np.mean([sum(x) for x in scores]), 2), \"with\", len(paths), \"paths\")\n",
    "                prev_j = (j + prev_j + 1)//2\n",
    "            if round_best_value > 0:\n",
    "                #print(round_best_value, sum(s[round_best_value_ind]), sum(scores[round_best_value_ind]))\n",
    "                paths[round_best_value_ind] = p[round_best_value_ind].copy()\n",
    "                scores[round_best_value_ind] = s[round_best_value_ind].copy()\n",
    "            else:\n",
    "                #print(\"crank up the j\")\n",
    "                if j > prev_j / 2:\n",
    "                    prev_j += 1\n",
    "                    break\n",
    "    return [(np.sum(scores[x]), scores[x], paths[x]) for x in range(len(paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_to_align(seq, sequence_matrix):\n",
    "    sequence = sequence_matrix[seq, :]\n",
    "    seq_length, align_length = sequence_matrix.shape\n",
    "    seq_to_align = []\n",
    "    cur = 0 \n",
    "    for i in range(align_length):\n",
    "        if \"-\" not in sequence[i]:\n",
    "            seq_to_align.append(i)\n",
    "    return seq_to_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sequence_to_matrix(match_matrix, pre_alignment=[], verbose=False):\n",
    "    seq_length, align_length = match_matrix.shape\n",
    "    alignment_score = np.sum([match_matrix[s, pre_alignment[s]] for s in range(seq_length)])\n",
    "    alignment_scores = [match_matrix[s, pre_alignment[s]] for s in range(seq_length)]\n",
    "    max_scores = np.max(match_matrix,axis=1)\n",
    "    norm_scores = alignment_scores / max_scores\n",
    "    fixed = [(x, pre_alignment[x]) for x in range(seq_length) if norm_scores[x]==1]\n",
    "    #print(pre_alignment)\n",
    "    if verbose:\n",
    "        print(\"Score to beat:\", round(alignment_score, 2), \"/\", seq_length)\n",
    "    pop_size = 50\n",
    "    \n",
    "    # Generating starting set\n",
    "    good, starts, ends = find_diverse_initial_population(match_matrix, pop_size=20, update_freq=50, \n",
    "                                                         max_rounds=100, verbose=verbose, fixed_sites=fixed)\n",
    "    \n",
    "    # Optimize weirdly\n",
    "    paths = [g[2] for g in good.copy()]\n",
    "    if verbose:\n",
    "        print(\"(1/2) Finding best start-end pairs...\")\n",
    "    starts_new, ends_new = get_best_start_ends(paths, match_matrix, prev_dicts=(starts.copy(), ends.copy()))\n",
    "    if verbose:\n",
    "        print(\"(1/2) Generating optimal paths from start-end pairs...\")\n",
    "    new_pop = generate_new_pop_from_starts_ends(starts_new, ends_new, len(paths))\n",
    "    paths = make_unique(paths)\n",
    "    prev_len = 0\n",
    "    if verbose:\n",
    "        print(\"Best paired:\", round(max(new_pop, key=lambda x:x[0])[0], 2))\n",
    "    while len(new_pop) > 1:\n",
    "        if len(paths) == prev_len:\n",
    "            break\n",
    "        prev_len = len(paths)\n",
    "        paths = [g[2] for g in new_pop.copy()]# + [g[2] for g in good.copy()] #+ [pre_alignment]\n",
    "        paths = make_unique(paths)\n",
    "        starts_new, ends_new = get_best_start_ends(paths, match_matrix, prev_dicts=(starts_new.copy(), ends_new.copy()))\n",
    "        new_pop = generate_new_pop_from_starts_ends(starts_new, ends_new, len(good))\n",
    "        if verbose:\n",
    "            print(\"Best paired:\", round(max(new_pop, key=lambda x:x[0])[0], 2))\n",
    "    \n",
    "    # Optimize randomly or something\n",
    "    full_pop = good.copy() + new_pop.copy() + [(np.sum(alignment_scores), alignment_scores, pre_alignment)]\n",
    "    refined_pop = random_refine(match_matrix, make_unique(full_pop), verbose=verbose, max_generations=30, fixed_sites=fixed)\n",
    "\n",
    "    # Optimize weirdly again\n",
    "    paths = [g[2] for g in refined_pop.copy()]\n",
    "    if verbose:\n",
    "        print(\"(2/2) Finding best start-end pairs...\")\n",
    "    starts_new, ends_new = get_best_start_ends(paths, match_matrix, prev_dicts=(starts.copy(), ends.copy()))\n",
    "    if verbose:\n",
    "        print(\"(2/2) Generating optimal paths from start-end pairs...\")\n",
    "    new_refined_pop = generate_new_pop_from_starts_ends(starts_new, ends_new, len(good))\n",
    "    paths = [g[2] for g in new_refined_pop.copy()] + [g[2] for g in good.copy()] + [pre_alignment]\n",
    "    paths = make_unique(paths)\n",
    "    prev_len = 0\n",
    "    if verbose:\n",
    "        print(\"Best paired:\", round(max(new_refined_pop, key=lambda x:x[0])[0], 2))\n",
    "    while len(new_refined_pop) > 1:\n",
    "        #print(len(paths))\n",
    "        if len(paths) == prev_len:\n",
    "            break\n",
    "        prev_len = len(paths)\n",
    "        paths = [g[2] for g in new_refined_pop.copy()]# + [g[2] for g in good.copy()] + [pre_alignment]\n",
    "        paths = make_unique(paths)\n",
    "        starts_new, ends_new = get_best_start_ends(paths, match_matrix, prev_dicts=(starts_new.copy(), ends_new.copy()))\n",
    "        #print(len(good))\n",
    "        new_refined_pop = generate_new_pop_from_starts_ends(starts_new, ends_new, len(good))\n",
    "        if verbose:\n",
    "            print(\"Best paired:\", round(max(new_refined_pop, key=lambda x:x[0])[0], 2))\n",
    "    new_refined_pop = make_unique(new_refined_pop)\n",
    "    new_refined_pop = [x for x in new_refined_pop if x[0] >= alignment_score]\n",
    "    #print(alignment_score, [x[0] for x in new_refined_pop])\n",
    "    return new_refined_pop\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutually_compatible(match_matrix, candidates, population):\n",
    "    population = [(c[0], c[1]) for c in population.copy()]\n",
    "    candidates = [(c[0], c[1]) for c in candidates.copy() if c not in population]\n",
    "    added = []\n",
    "    while len(candidates) != 0:\n",
    "        ind = np.random.randint(len(candidates))\n",
    "        s, a = candidates[ind]\n",
    "        upper_limits = [x for x in population if x[0] > s] + [[match_matrix.shape[0], match_matrix.shape[1]]]\n",
    "        lower_limits = [x for x in population if x[0] < s] + [[0, 0]]\n",
    "        site_range = (max([x[1] for x in lower_limits]), min([x[1] for x in upper_limits]))\n",
    "        if (site_range[0] > site_range[1]) or (a not in list(range(site_range[0], site_range[1]))) or (a in [x[1] for x in population]):\n",
    "            #print(a, list(range(site_range[0], site_range[1])))\n",
    "            candidates.pop(ind)\n",
    "        elif site_range[1]-a < min(x[0] for x in upper_limits)-s or a-site_range[0] < s-max([x[0] for x in lower_limits]):\n",
    "            candidates.pop(ind)\n",
    "        else:\n",
    "            #print(a, list(range(site_range[0], site_range[1])))\n",
    "            population.append(candidates[ind])\n",
    "            added.append(candidates.pop(ind))\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_monotonic(site_list):\n",
    "    prev_s, prev_a = site_list[0]\n",
    "    for i in range(1, len(site_list)):\n",
    "        s, a = site_list[i]\n",
    "        if s <= prev_s or a <= prev_a:\n",
    "            return False\n",
    "        prev_s, prev_a = s, a\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
